You will fill this file with all your work from the point of view of the present! 

```{r}
library(tidyverse)
library(magrittr)
library(patchwork)

library(lubridate)

library(tsibble)
library(feasts)
library(forecast)

library(sandwich)
library(lmtest)

library(nycflights13)
library(blsR)
library(dplyr)
library(readr)
library(fable)
library(gridExtra)
```

```{r Sandeep Work}
co2_tsb <- co2 %>% as_tsibble()
colnames(co2_tsb)[2] <- "co2_ppm"

co2_1997_linear <- co2_tsb %>% model(TSLM(co2_ppm ~ trend()))
report(co2_1997_linear)
augment(co2_1997_linear) %>%
  ggplot(aes(x=index)) +
  geom_line(aes(y=.fitted, colour = 'LinearFit')) +
  geom_line(aes(y=co2_ppm, colour = 'Original')) +
  labs(title='CO2 ppm vs. Linear Model') +
  scale_colour_manual(values = c(LinearFit = 'red', Original = 'blue')) +
  guides(colour = guide_legend(title = 'Legend'))
# co2_1997_linear %>% 
#   gg_tsresiduals() +
#   labs(title = "Residuals of a Linear Model fit")
```


```{r Sandeep Work}
co2_tsb <- co2 %>% as_tsibble()
colnames(co2_tsb)[2] <- "co2_ppm"
head(co2_tsb)

co2_1997_linear <- co2_tsb %>%
  model(TSLM(co2_ppm ~ trend()))
report(co2_1997_linear)
augment(co2_1997_linear) %>%
  ggplot(aes(x=index)) +
  geom_line(aes(y=.fitted, colour = 'LinearFit')) +
  geom_line(aes(y=co2_ppm, colour = 'Original')) +
  labs(title='CO2 ppm vs. Linear Model') +
  scale_colour_manual(values = c(LinearFit = 'red', Original = 'blue')) +
  guides(colour = guide_legend(title = 'Legend'))
# co2_1997_linear %>% 
#   gg_tsresiduals() +
#   labs(title = "Residuals of a Linear Model fit")
```


Loading and transforming the dataset. 
```{r Finnian Work}
dataw1 <- read.table("https://gml.noaa.gov/webdata/ccgg/trends/co2/co2_weekly_mlo.txt", header=FALSE)
dataw1 <- dataw1 %>% select(V1, V2, V3, V5) %>% rename("year"=V1, "month"=V2, "day"=V3, "co2"=V5) %>% filter(co2>0)
dataw1$year_month <- lubridate::make_datetime(year=dataw1$year, month=dataw1$month, tz='UTC') 
dataw <- dataw1 %>% select(year_month, co2) %>% mutate(year_month = as.Date(year_month, "%Y-%m"))
dataw <- aggregate(dataw$co2, by=list(year_month=dataw$year_month), FUN=mean) %>% rename("time_index"=year_month, "CO2_avg"=x)
co2_present <- dataw %>% mutate(time_index = yearmonth(as.character(time_index)))  %>% as_tsibble(index=time_index)

co2_present <- tsibble::fill_gaps(co2_present, CO2_avg=330.5)

##basic plots
co2_present %>% gg_tsdisplay(CO2_avg, plot_type="partial") +
  ylab(TeX(r'($CO_2$ parts per million)')) +
  ggtitle("Time Series, Autocorrelation (acf), Partial Autocorrelation(pacf)")

##examining seasonality
co2_present %>% gg_season(CO2_avg) +
  ylab(TeX(r'($CO_2$ parts per million)')) +
  ggtitle("Annual Seasonality")

co2_present %>%
  gg_subseries(CO2_avg) +
  ylab(TeX(r'($CO_2$ parts per million)')) +
  ggtitle("Seasonal Sub-series for each month")

plot.ts(co2_present$CO2_avg, col = 'red', type = 'l', 
     xlab = "Year (time period: month)", ylab = "CO2 ppm", 
     main = TeX(r'(Time-Series plot of $CO_2$ concentration)'))
abline(h = mean(co2_present), col = 'green', lty = 2)
lines(stats::filter(co2_present$CO2_avg, sides=2, rep(1, 12)/12), lty = 1, 
      lwd = 1.5, col = "blue")
leg.txt <- c("Time-series", "Mean value", 
             "12-Month Symmetric Moving Average")
legend("topleft", legend = leg.txt, lty = c(1, 2, 1), lwd = c(1, 1, 1.5), 
       col = c("red", "green", "blue"), bty = 'n', cex = .8)

##decomposition

co2_decomp <- co2_present %>%
  model(
    classical_decomposition(CO2_avg, type = "additive")
  ) %>%
  components() %>%
  autoplot() +
  labs(title = "Classical additive decomposition")

sum(is.na(co2_present$CO2_avg))
co2_decomp

##examining just the section above 1979

co2_1998_plus <- dataw1 %>% select(year, month, co2) %>% filter(year > 1997)
co2_1998_plus$time_index <- lubridate::make_datetime(year=co2_1998_plus$year, month=co2_1998_plus$month, tz='UTC') 
co2_1998_plus <- co2_1998_plus %>% select(time_index, co2) %>% mutate(time_index = as.Date(time_index))
co2_1998_plus <- aggregate(co2_1998_plus$co2, by=list(time_index=co2_1998_plus$time_index), FUN=mean) %>% rename("CO2_avg"=x)
co2_1998_plus <- co2_1998_plus %>% mutate(time_index = yearmonth(as.character(time_index))) %>% as_tsibble( index=time_index)

co2_1998_plus %>% gg_tsdisplay(CO2_avg, plot_type="partial") +
  ylab(TeX(r'($CO_2$ parts per million)')) +
  ggtitle("Time Series, Autocorrelation (acf), Partial Autocorrelation(pacf)")


## decomposing

co2_decomp <- co2_present %>%
  model(
    classical_decomposition(CO2_avg, type = "additive")
  ) %>%
  components() %>%
  autoplot() +
  labs(title = "Classical additive decomposition")

# detrending the data
trend <- lm(co2_present$CO2_avg ~ co2_present$time_index)
summary(trend)
detrend <- residuals(trend)
ts.plot(detrend, xlab="Month", ylab="Detrended C02 Levels")

## detrending - differencing
acf(detrend)
pacf(detrend)
differenced <- diff(co2_present$CO2_avg)
ts.plot(differenced)
acf(differenced)
pacf(differenced)
Box.test(differenced, type="Ljung")


##differencing and seasonal differencing

co2_present_annual_plus1<- co2_present$CO2_avg %>%
  diff(lag=12) %>%
  diff(lag=1)

Box.test(co2_present_annual_plus1, type="Ljung")
```
## Introduction
The dataset is pulled from the NOAA Global Monitoring Laboratory website for the monthly average CO2 levels in Mauna Loa. Data from March 1958 through april 1974 have been obtained by C David Keeling of the Scripps Insititution of Oceanography. Weekly CO2 values are constructed from daily mean values. NOAA has "confidence that the CO2 measurements made at the Mauna Loa observatory reflect truth about our global atmosphere," given that the observatory is at the summit of Mauna Loa at an altitude of 3400 meters and can measure air masses that are representative of large areas, the measurements are frequently and rigorously calibrated, and ongoing comparisons are made to ensure data accuracy. 
The NOAA GML measures the "mole fraction" of CO2 in dry air, the number of CO2 molecules in a given number of molecules of air after the removal of water vapor. This process is done given that the dry mole fraction reflects the addition and removal of the gas given that dry air does not change when air expands upon heating or ascending to a higher altitude where the pressure is lower. 

The data is missing values for one week in 1954, and that value has been replaced by the average of its surrounding values on each side. Monthly averages are constructed by taking simple averages of values across each month of the time series. 

From plotting the time series of the data, we see a strong upward trend and likely aspects of seasonality given the oscillating pattern of the time series throughout the trend. 
From the ACF of the CO2, we see high (and statistically significant) correlation for many lags (100 lags shown on the above plot). Significance is maintained through ~250 lags. This high level of autocorrelation is indicative of trended data. 
Observing the PACF plot, we see no statistically significant values past the first lag. 
Examining just the years after 1979, we see a fluctuation of autocorrelations on the ACF plot, dipping below statistical significant at ~75 lags. The PACF plot shows statistically significant values ocurring for the first two lags and then at additional lags further out. 
From the linear model of the co2 levels on the time index, we see a highly statistically significant linear relationship between the two variables, as expected from the trends observed in the time series plot and the results of the ACF plot. 
To examine the detrending data, we will look at the residuals of the trend model. 
We see a parabolic shape and continued seasonality. 
We still see highly statistically significant autocorrelation in the ACF plot, and PACF values that remain statistically significant. 
The differenced time series appears more like a white noise model. 
Even in the differenced data, we still see statistically significant autocorrelations, which is not indicative of a white noise model. 
The Ljung-Box test has a very small p value, indicating that we should reject the null hypothesis that the residuals of our time series model are independent. 



## Task 2b: Compare linear model forecasts against realized CO2

```{r Compare Linear Model}
set.seed(44)
colnames(co2_1998_plus)[2] <- "co2_ppm"

fit <- co2_tsb %>% model(TSLM(co2_ppm ~ trend() + I(trend()^2) + season()))

simulation <- fit %>% forecast(h = 295)
simulation.with.bootstrap <- fit %>% forecast(h = 295, times = 50, bootstrap = TRUE)

aug <- fit %>% augment()
print("Innnovation Residual Mean")
print(mean(aug$.resid))
qqnorm(aug$.resid, pch = 1, frame = FALSE, xlab = "Fitted", ylab = "Residual") 
fit %>% gg_tsresiduals()

p1<-co2_tsb %>%
  ggplot(aes(x = index)) +
  geom_line(aes(y = co2_ppm)) +
  geom_line(aes(y = .mean, colour="red"), data = simulation) +
  labs(title="CO_2 forecast - Linear", y="co_2" ) +
  guides(colour = "none")

p2<-co2_tsb %>%
  ggplot(aes(x = index)) +
  geom_line(aes(y = co2_ppm)) +
  geom_line(aes(y = .mean, colour="red"), data = simulation.with.bootstrap) +
  labs(title="CO_2 forecast with Bootstrap - Linear", y="co_2" ) +
  guides(colour = "none")
grid.arrange(p1,p2, nrow=2)
print("Accuracy of trend + season")
colnames(co2_1998_plus)[1] <- "index"
accuracy(simulation$.mean, co2_1998_plus$co2_ppm)
print("Accuracy of trend + season with Bootstrap")
accuracy(simulation.with.bootstrap$.mean, co2_1998_plus$co2_ppm)

accuracy(simulation, co2_1998_plus,list(winkler = winkler_score), level = 80)
accuracy(simulation.with.bootstrap, co2_1998_plus, list(winkler = winkler_score), level = 80)
accuracy(simulation, co2_1998_plus, list(winkler = winkler_score), level = 95)
accuracy(simulation.with.bootstrap, co2_1998_plus, list(winkler = winkler_score), level = 95)

n <- 36
tail.actual <- tail(co2_1998_plus$co2_ppm, n)
tail.pred <- tail(simulation$.mean, n)
print("RMSE of Last 36 months")
print(sqrt(mean((tail.actual-tail.pred)^2)))

head.actual <- head(co2_1998_plus$co2_ppm, n)
head.pred <- head(simulation$.mean, n)
print("RMSE of First 36 months")
print(sqrt(mean((head.actual-head.pred)^2)))
```
**Linear Model Explanation**

To begin with let's take a look at the residual of the fitted linear model. A 
good forecasting method will yield innovation residuals with the following properties:

1) The innovation residuals are uncorrelated. If there are correlations between 
innovation residuals, then there is information left in the residuals which should 
be used in computing forecasts.

2) The innovation residuals have zero mean. If they have a mean other than zero, 
then the forecasts are biased.The innovation residuals also fails to show us the 
white noise. Thus we anticipate the forecast-ed values to not reflect the full 
absorption of trend and seasonality.

3) The innovation residuals have constant variance. This is known as “homoscedasticity”.

4) The innovation residuals are normally distributed.

Looking at the ACF, there is a strong auto-correlation between the preset values 
of the series and the lagged values. So we conclude the residuals are correlated.
The Innovation mean is close to zero. The innovation residuals have near constant 
variance. The innovation residuals are normally distributed in the histogram plot.

As the residual still have trend/seasonality left in it, while forecasting, the
team explored both with **Residual Re-sampling** via bootstrap residuals and without
boot-straping residuals to explore how well the model behaves. 

As expected the forecast-ed model have a few positives and pitfalls. 

1) The Seasonality is consistently under estimated. This is evident from the height
of the seasonality smaller than the training data in both boot-strapped and 
non-boot-strapped forecasts.

2) The RMSE and other error metrics are reasonably small.

3) To evaluate the distributional forecast accuracy our team employed **Winkler Score** 
test. The Winkler score can be interpreted like an absolute error. For observations 
that fall within the interval, the Winkler score is simply the length of the interval. 
So low scores are associated with narrow intervals. However, if the observation 
falls outside the interval, the penalty applies, with the penalty proportional to 
how far the observation is outside the interval. A score of 2.6 for 80% confidence
interval and 3.28 for 95% confidence interval shows the forecast-ed values are 
not way off.

4) The trend is not captured well. This is evident from the **90.6% increase** in the 
RMSE values for the last 36 months compared to the first 36 months - 1.18415 vs 
0.6213753. Thus, the small unobserved trend in the residuals for the initial forecast-ed 
values create a large effect in the far future forecasts - **the butterfly effect** is the 
sensitive dependence on initial conditions in which a small change in one state of a 
deterministic nonlinear system can result in large differences in a later state.

Based on the above observations, **the best linear model consistently under predicts**.

## Task 3b: Compare ARIMA models forecasts against realized CO2

```{r Compare ARIMA Model}
set.seed(44)
colnames(co2_1998_plus)[2] <- "co2_ppm"

fit <- co2_tsb %>% model(ARIMA(co2_ppm ~ 0 + pdq(7,1,8) + PDQ(1,1,1), 
                               stepwise=FALSE, approximation=FALSE)) 

simulation <- fit %>% forecast(h = 295)
simulation.with.bootstrap <- fit %>% forecast(h = 295, times = 50, bootstrap = TRUE)

aug <- fit %>% augment()
print("Innnovation Residual Mean")
print(mean(aug$.resid))
qqnorm(aug$.resid, pch = 1, frame = FALSE, xlab = "Fitted", ylab = "Residual") 
fit %>% gg_tsresiduals()

p1<-co2_tsb %>%
  ggplot(aes(x = index)) +
  geom_line(aes(y = co2_ppm)) +
  geom_line(aes(y = .mean, colour="red"), data = simulation) +
  labs(title="CO_2 forecast - Linear", y="co_2" ) +
  guides(colour = "none")

p2<-co2_tsb %>%
  ggplot(aes(x = index)) +
  geom_line(aes(y = co2_ppm)) +
  geom_line(aes(y = .mean, colour="red"), data = simulation.with.bootstrap) +
  labs(title="CO_2 forecast with Bootstrap - Linear", y="co_2" ) +
  guides(colour = "none")
grid.arrange(p1,p2, nrow=2)
print("Accuracy of trend + season")
colnames(co2_1998_plus)[1] <- "index"
accuracy(simulation$.mean, co2_1998_plus$co2_ppm)
print("Accuracy of trend + season with Bootstrap")
accuracy(simulation.with.bootstrap$.mean, co2_1998_plus$co2_ppm)

accuracy(simulation, co2_1998_plus,list(winkler = winkler_score), level = 80)
accuracy(simulation.with.bootstrap, co2_1998_plus, list(winkler = winkler_score), level = 80)
accuracy(simulation, co2_1998_plus, list(winkler = winkler_score), level = 95)
accuracy(simulation.with.bootstrap, co2_1998_plus, list(winkler = winkler_score), level = 95)

n <- 36
tail.actual <- tail(co2_1998_plus$co2_ppm, n)
tail.pred <- tail(simulation$.mean, n)
print("RMSE of Last 36 months")
print(sqrt(mean((tail.actual-tail.pred)^2)))

head.actual <- head(co2_1998_plus$co2_ppm, n)
head.pred <- head(simulation$.mean, n)
print("RMSE of First 36 months")
print(sqrt(mean((head.actual-head.pred)^2)))
```
**ARIMA Model Explanation**

To begin with let's take a look at the residual of the fitted linear model. A 
good forecasting method will yield the above mentioned innovation residual properties.

ACF strongly reflects white noise. So we conclude the residuals are not correlated.
The Innovation mean is close to zero, but substantially higher than the linear model. 
The innovation residuals have near constant variance. The innovation residuals are 
normally distributed in the histogram plot.

The forecast-ed model is not exhibiting substantial difference to the Linear model.

1) The Seasonality is estimated better than the Linear model. This is evident from the height
of the seasonality relatively closer to the training data in both boot-strapped and 
non-boot-strapped forecasts.

2) The RMSE and other error metrics are high. As a matter of fact, all the error metrics
are higher than the Linear model.

3) To evaluate the distributional forecast accuracy our team employed **Winkler Score** 
test. A score of 37.3 for 80% confidence interval and 47.13 for 95% confidence interval 
shows the forecast-ed values are way off.

4) The trend is not captured well. This is evident from the **1200%** increase in the 
RMSE values for the last 36 months compared to the first 36 months - 16.19685 vs 
1.187938. Thus, the small unobserved trend in the residuals for the initial forecast-ed 
values create a large effect in the far future forecasts - **the butterfly effect**.

Based on the above observations, **the ARIMA model consistently under predicts** and
no better than the linear model predictions.

???Describe how the Keeling Curve evolved from 1997 to the present.???

## Task 4b: Evaluate the performance of 1997 linear and ARIMA models

```{r Evaluate the performance of 1997 linear and ARIMA models}
arima.fit <- co2_tsb %>% model(ARIMA(co2_ppm ~ 0 + pdq(7,1,8) + PDQ(1,1,1),
                               stepwise=FALSE, approximation=FALSE))
arima.sim <- arima.fit %>% forecast(h = 424)
tail(arima.sim$.mean)

linear.fit <- co2_tsb %>% model(TSLM(co2_ppm ~ trend() + I(trend()^2) + season()))
linear.sim <- linear.fit %>% forecast(h = 424)
tail(linear.sim$.mean)

linear.sim.295 <- linear.fit %>% forecast(h = 295)
arima.fit.295 <- arima.fit %>% forecast(h = 295)
all.forecast <- bind_cols(co2_1998_plus, linear.sim.295$.mean, arima.fit.295$.mean)

colnames(all.forecast)[3] <- "linear"
colnames(all.forecast)[4] <- "arima"
forecast.resi <- all.forecast %>%
  mutate(linear.residual = co2_ppm-linear) %>%
  mutate(arima.residual  = co2_ppm-arima)

ttest.resi <- t.test(forecast.resi$linear.residual, 
                     forecast.resi$arima.residual, alternative = c("two.sided"))
print("Residual t-test Result")
print(ttest.resi)

n <- 36
tail.data <- tail(forecast.resi, n)
head.data <- head(forecast.resi, n)
print("RMSE of Last 36 months - ARIMA")
print(sqrt(mean((tail.data$arima.residual)^2)))
print("RMSE of First 36 months - ARIMA")
print(sqrt(mean((head.data$arima.residual)^2)))

print("RMSE of Last 36 months - Linear")
print(sqrt(mean((tail.data$linear.residual)^2)))
print("RMSE of First 36 months - Linear")
print(sqrt(mean((head.data$linear.residual)^2)))

print("Accuracy of Linear Model")
accuracy(forecast.resi$linear, co2_1998_plus$co2_ppm)
print("Accuracy of ARIMA Model")
accuracy(forecast.resi$arima, co2_1998_plus$co2_ppm)

Box.test(forecast.resi$linear.residual, lag = 1, type = c("Ljung-Box"), fitdf = 0)
Box.test(forecast.resi$arima.residual, lag = 1, type = c("Ljung-Box"), fitdf = 0)
```
**Explanation**

???Explain how the model meets 420ppm???

The forecasting performance of the models are assessed by a test and two metrics.

1) **t-test** and **Box test**

2) **The Butterfly effect**

3) **Error Metrics**

1) t-test determines whether the means of two groups are equal to each other.
True difference in means is not equal to 0 for the residuals of Linear and Arima 
tells us that the models have different means

2)  **The butterfly effect** is the sensitive dependence on initial conditions 
in which a small change in one state of a deterministic nonlinear system can result 
in large differences in a later state. A **90.6% increase** in the 
RMSE values for the last 36 months compared to the first 36 months - 1.18415 vs 
0.6213753 of the linear model proves the trend is not fully captured. Thus, 
the small unobserved trend in the residuals for the initial forecast-ed 
values create a large effect in the far future forecasts. Also, a **1200%** increase in the 
RMSE values for the last 36 months compared to the first 36 months - 16.19685 vs 
1.187938 for the ARIMA model echoes the same.

3) ME, RMSE, MAE, MPE, MAPE: ???



```{r Finnian Work 2.5}
##Seasonally adjust the weekly nOAA data 
##lagging at 12 for annual seasonal, can change to other number for annual "seasons"
season_num <- 12
co2_present$seasonally_adjusted_co2 <- difference(co2_present$CO2_avg, lag=season_num)

##split into seasonally adjusted and nonseasonally adjusted
SA_co2_present <- co2_present %>% select(time_index, seasonally_adjusted_co2)
NSA_co2_present <- co2_present %>% select(time_index, CO2_avg)

##split test and training sets
SA_co2_present_train <- SA_co2_present %>% filter(time_index < yearmonth("2020-08-01"))
SA_co2_present_test <- SA_co2_present %>% filter(time_index > yearmonth("2020-07-01"))

NSA_co2_present_train <- NSA_co2_present %>% filter(time_index < yearmonth("2020-08-01"))
NSA_co2_present_test <- NSA_co2_present %>% filter(time_index > yearmonth("2020-07-01"))


##fit ARIMA models for SA and forecasts

sa_present_fit_best <- auto.arima(y=SA_co2_present_train$seasonally_adjusted_co2, max.p=12, max.P=12, max.Q=0, max.d=2, max.q = 0, max.D=2)


sa_forecast <- forecast(SA_co2_present_train$seasonally_adjusted_co2, model=sa_present_fit_best, h=24)

plot(sa_forecast)


nsa_present_fit_best <- auto.arima(y=NSA_co2_present_train$CO2_avg, max.p=12, max.P=12, max.Q=0, max.d=2, max.q = 0, max.D=2)



nsa_forecast <- forecast(NSA_co2_present_train$CO2_avg, model=nsa_present_fit_best, h=24)

plot(nsa_forecast)

##fits

fit_NSA <- NSA_co2_present_train %>% model(TSLM(CO2_avg ~ trend()+season())) %>% forecast(h="2 years")
fit_SA <- SA_co2_present_train %>% model(TSLM(seasonally_adjusted_co2 ~ trend()+season())) %>% forecast(h="2 years")


##accuracies 

accuracy(fit_NSA, NSA_co2_present_test)
accuracy(fit_SA, SA_co2_present_test)

```
