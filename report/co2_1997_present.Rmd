---
title: "Present and Future Global $CO_{2}$ Emissions"
short: "What Keeling missed all these years"
author:
  - name: Finnian Meagher
    firstname: Finnian
    surname: Meagher
    email: fmeagher@berkeley.edu
    affiliation: UC Berkeley, School of Information
  - name: Kumar Narayanan
    firstname: Kumar
    surname: Narayanan
    email: kumarn@ischool.berkeley.edu
    affiliation: UC Berkeley, School of Information
  - name: Sandeep Kataria
    firstname: Sandeep 
    surname: Kataria
    email: kataria@berkeley.edu
    affiliation: UC Berkeley, School of Information
  - name: Satheesh Joseph
    firstname: Satheesh 
    surname: Joseph
    email: satheeshrishi@berkeley.edu
    affiliation: UC Berkeley, School of Information
acknowledgements: | 
  The authors would like to thank their instructors from MIDS 271.
abstract: | 
  Year 1998 is upon us and global attention is turning toward the consequences of human-actions in our environmental system. While industrial pollution, water contamination, and others have their own impacts on health, none affects the entire globe as much as the global rise in temperature. The Intergovernmental Panel on Climate Change (IPCC) has been examining these trends for more than ten years. In the second report released in 1995 the IPCC notes that the balance of the evidence suggests that climate changes, in particular global warming, is attributable to human activities. Skepticism on the findings, that the global temperature rise due to human activities, exists but hasn't yet degenerated in a polarized pro- and anti- global warming camps. That said, there is also little political will to mitigate the global warming effects through tangible actions and/or policies. The effect of $CO_{2}$ causing "Green House" effect is no more under debate; nor the effect of Green House to raise the temprature. This behooves us to analyze the $CO_{2}$ levels in the atmosphere. Data from the Mona Loa Observatory (MLO) is analyzed in this report to describe and predict global $CO_{2}$ concentrations under several possible scenarios. What we find, when we run the analysis, may paint a grim picture. 
header-includes: 
  - '\usepackage{graphicx}'
  - '\usepackage{booktabs}'
output: pdf_document
---

```{r setup, echo=FALSE}
## default to not show code, unless we ask for it.
knitr::opts_chunk$set(echo=FALSE)
options(digits = 3)

r = getOption("repos")
r["CRAN"] = "http://cran.us.r-project.org"
options(repos = r)
```

```{r load packages, echo=FALSE, message=FALSE, include=FALSE}
library(tidyverse)
library(tsibble)
library(tseries)
library(latex2exp)
library(ggplot2)
library(patchwork)
library(magrittr)
library(lubridate)
library(feasts)
library(forecast)
library(fable)
library(sandwich)
library(lmtest)
library(gridExtra)
library(scales)
library(zoo)
library(plyr)
library(dplyr)
library(tidyr)
library(gridExtra)

install.packages("pander")
library(pander)

install.packages("reshape2")
library(reshape2)

install.packages("pastecs")
library(pastecs)

install.packages("car")
library(car)

install.packages("ggfortify")
library(ggfortify)

install.packages("stargazer")
library(stargazer)

install.packages("astsa")
library(astsa)

install.packages("ggthemes")
library(ggthemes)

install.packages("gtools")
library(gtools)

install.packages("kableExtra")
library(kableExtra)

theme_set(theme_minimal())
knitr::opts_chunk$set(dpi=1000)
```

For the last several hundreds of years we earthlings have been living under a "Golden Period" of moderate climactic conditions, barring few locations that have extremes. The vast majority of the earth is inhabitable. We enjoy regular seasons which allows for a steady stream of food, fresh water, flora, and fauna. Overall, we have had a balanced environment around us. Increased human activities and the desire to exploit nature at an ever increasing rate for our needs have impacted this balance and, consequently, the climate that we otherwise take for granted. Understanding changing climate, and what it means for the earth's inhabitants is of growing interest to the scientific and policy community. We do not as yet reckoned all the effects of the human actions on climate. One of the key stabilizing parameters is the temperature of the earth itself. Huge quantities of fresh water are frozen in the form of glaciers. An increase in temperature, for instance, can melt these glaciers and cause water levels to raise. The impact of this action on coastal community is a disaster, to put it mildly. There are several other disastrous outcomes that temperature increase can cause. One of the main causes for temperature increase is the green house effect of gases in our atmosphere, mainly in the form of $CO_{2}$. This report analyses what we've seen so far in the carbon footprint in our atmosphere, and what possible predictions we can make. We hope that this analysis provides an understanding of how $CO_{2}$ level has changed and what the forecast for $CO_2$ level is if the current dynamics continue. We hope that this motivates us to take actions to reduce $CO_{2}$ emissions. 

# Background 
## Carbon Emissions 
What do we mean by carbon emission, and why do we care about this? The term carbon emission is used in the context of how carbon, in the form of Carbon Dioxide ($CO_2$), in our atmosphere impacts our lives. A lot of carbon is sequestered in the form of plants. Plants consume $CO_2$, along with water and sunlight, to produce glucose in a process called Photosynthesis. Thus, not only carbon stays in plants but plants remove $CO_2$ from the atmosphere. When we lose plant life on earth we're denied of the $CO_2$ cleaning mechanism. Carbon is also present in fossil fuel and coal. When we burn fossil fuel, coal, and wood for our energy needs we release $CO_2$. Nature maintains a delicate balance where $CO_2$ emissions in moderation can be consumed by the plants. When we destroy forests to expand our habitable land and burn fossil fuel, coal, and wood we're creating an avalanche effect. 

There is little dispute that burning fossil fuel releases $CO_2$. What we have understood is that $CO_2$ is a Green House gas. $CO_2$ acts like a glass which allows light to get through while trapping heat. A simple analogy is that when we sit inside a car, windows drawn up, on a bright cold day, we can feel the temperature inside the car rise. The light energy coming through the glass window heats up the air in the car and the heat is trapped by the glass. This phenomenon is called Green House effect, named after the techniques used to grow vegetables inside glass enclosures in colder climates. $CO_2$ plays the role of the glass around the earth. Thus, more $CO_2$ more heat is trapped. 

While there are other green house gases such as Methane ($CH_4$) which are far more damaging than $CO_2$ our focus is on $CO_2$. Methane has relatively lower concentration and its emission is not as prevalent as that of $CO_2$. Hence, we focus the rest of the report to understand the trend we have seen thus far of $CO_2$ levels in the atmosphere, and what it forebodes. 

# Measurement and Data 
## Measuring Atmospheric Carbon 

Crucial to studying trends and forecasting levels of atmospheric carbon is reliable measurement of this concept. The importance of measuring $CO_2$ levels at relatively isolated places is important to understand the $CO_2$ levels in our atmosphere. For instance, measuring $CO_2$ in the vicinity a refinery or some other factories will provide very skewed readings. We may also see huge variance depending on the load the factory is handling. Similarly, measuring $CO_2$ levels in a busy downtown thoroughfare is also avoidable. Clearly, we will see elevated levels, and the measurements depend on the commute hours, holidays etc. 

Thus, we need a neutral ground to measure the $CO_2$ levels. The data we are analyzing comes from Mouna Loa Observatory (MLO) in Hawaii's Big Island. At a height of over $13,500$ feet on a land that is surrounded by the Pacific Ocean on all sides, with no major industries, no major automobile movement in the area, and surrounded by nature, we believe that the data from MLO provides reasonable measurements. While there are active volcanoes in Hawaii in the Big Island, the altitude of the MLO allows for a neutral ground to measure $CO_2$ levels.

## Historical Trends in Atmospheric Carbon 
Atmospheric carbon is plotted in **Monthly Mean $CO_2$**, and shows some worrying trends. As is evident in the plot titled $Monthly$ $Mean$ $CO_2$ we see an increasing trend. This plot is referred to as the **The Keeling Curve**, named after the scientist Dr. Charles David Keeling, shows seasonal variations but the trend is definitely upward.

```{r plot the keeling curve, echo=FALSE, fig.dim=c(8,3)}
tsibble::as_tsibble(co2) %>%
  ggplot() + 
  aes(x=index, y=value) + 
  geom_line(color = 'steelblue') +
  labs(
    title = TeX(r'(Monthly Mean $CO_2$)'),
    subtitle = 'The "Keeling Curve"',
    x = 'Month and Year',
    y = TeX(r'($CO_2$ parts per million)')
  )
```

We will further examine the MLO data to understand better what we have observed till date, and what the forecast may look like. Let us look at the first few observations of the data.

```{r sneak peek, echo=FALSE}
head(co2, 24)
```

```{r generate the required data object, include=FALSE}
# generate tsibble object - most conducive for analysis
co2_tsb <- co2 %>% as_tsibble()
colnames(co2_tsb)[2] <- "co2_ppm"
```
```{r check for missing values, echo=FALSE}
# check for missing values
num_na <- sum(is.na(co2_tsb$co2_ppm))
if (num_na > 0) {
  print(paste("There are", num_na, "missing values"))
}
```

The data, when organized as a table indexed by month, has `r dim(co2_tsb)[2]` columns, named **index** and **co2_ppm**. The column names are somewhat self explanatory - the index of this table is the month of each year, and for each month we've the mean $CO_2$ level in Parts Per Million (ppm). There are `r dim(co2_tsb)[1]` observations, starting from `r as.character(min(co2_tsb$index))` and ending at `r as.character(max(co2_tsb$index))`.

Let us examine the data further. Let's look at the time series data, the correlation among observations, referred to as Serial Correlation of Autocorrelation (ACF), and also the Partial Correlation (PACF), which is the correlation between two readings separated by lag 'K' after removing the observations that are between the two observations being correlated.

```{r EDA explore time-series and basic characteristics, echo=FALSE, fig.dim=c(8,4)}
co2_tsb %>% gg_tsdisplay(co2_ppm, plot_type="partial") +
  ylab(TeX(r'($CO_2$ parts per million)')) +
  ggtitle("Time Series, Autocorrelation (acf), Partial Autocorrelation(pacf)")
```

The time series graph is same as the Keeling Curve seen above. A visual analysis of the ACF indicates possible seasonality. The ACF values are not monotonically decreasing. We see a wavy pattern, indicative of possible seasonality. We'll drill down further to examine the seasonality.

```{r EDA explore seasonality and monthly subseries , echo=FALSE, fig.dim=c(8,3)}
p1<-co2_tsb %>% gg_season(co2_ppm) +
  ylab(TeX(r'($CO_2$ parts per million)')) +
  ggtitle("Annual Seasonality")
p2<-co2_tsb %>%
  gg_subseries(co2_ppm) +
  ylab(TeX(r'($CO_2$ parts per million)')) +
  ggtitle("Seasonal Sub-series for each month")
grid.arrange(p1,p2, nrow = 1, ncol = 2)
```

In the plot above, titled **Annual Seasonality** we see an almost same degree of seasonality year after year. Two pieces of information are evident in the plot - there is a clear seasonality in the amount of $CO_2$ that hits a peak in the mid-May to mid-June time frame, and hits a minimum in the late-September to early October time frame, and the $CO-2$ levels have been going up without exception year over year. As [figured out by Dr. Keeling](https://scrippsco2.ucsd.edu/assets/publications/keeling_tellus_1960.pdf) the plant life in the Northern Hemisphere starts growing in late spring onward which causes $CO_2$ to be absorbed by them. When the fall season kicks in the leaves fall and the plants stop growing leading to higher concentration both because of lack of vegetation to absorb $CO_2$ and also in small part by the fallen leaves and vegetation.

Additionally, let us examine the sub-series. This plot shows us how each month fared across years of observation. In the plot titled **Seasonal Sub-series for each month** we see that the trend we examined earlier is clearly visible. The blue lines, that represent the mean for the month across all years of observations show a peak in the May-June time frame and a low in September-October time frame. It is also quite evident that for a given month the amount of $CO_2$ has monotonically increase from 1959 to 1997.

We can also take a quick look at the histogram to see if any obvious pattern emerges.

```{r EDA histogram, echo=FALSE, fig.dim=c(8,3)}
co2_tsb %>% 
  ggplot(aes(x=co2_ppm)) +
  xlab(TeX(r'($CO_2$ parts per million)')) +
  geom_histogram(bins=30, color="black", fill="white") 
```

The histogram is not particularly interesting. Given the seasonality the value of around 320ppm may appear more often. 

```{r EDA yearly trend, echo=FALSE, fig.dim=c(8,3)}
plot.ts(co2_tsb$co2_ppm, col = 'red', type = 'l', 
     xlab = "Year (time period: month)", ylab = "CO2 ppm", 
     main = TeX(r'(Time-Series plot of $CO_2$ concentration)'))
abline(h = mean(co2), col = 'green', lty = 2)
lines(stats::filter(co2_tsb$co2_ppm, sides=2, rep(1, 12)/12), lty = 1, 
      lwd = 1.5, col = "blue")
leg.txt <- c("Time-series", "Mean value", 
             "12-Month Symmetric Moving Average")
legend("topleft", legend = leg.txt, lty = c(1, 2, 1), lwd = c(1, 1, 1.5), 
       col = c("red", "green", "blue"), bty = 'n', cex = .8)
```

The plot titled **Time-series plot of the** $CO_2$ **concentration** shows the time series and the trend together. The visual clearly shows the increasing trend in the $CO_2$ level as a function of time. 

We can get a visual of how the $CO_2$ level varied across years.

```{r EDA box plot, echo=FALSE, fig.dim=c(8,3)}
boxplot(co2 ~ factor(rep(1959:1997, each = 12)), 
        xlab = 'Year', ylab = 'CO2 PPM',
        outcex = 0.8, medcol="blue", lwd = 0.3, 
        main = TeX(r'(Annual Variation of $CO_2$ concentration)'))
```

As we examine data we want to see the trend component, seasonal component, and the reminder or random component. We use two different methods - classical decomposition and STL (Seasonal and Trend decomposition using Loess).

```{r EDA decompose into trend and season, echo=FALSE, warning=FALSE, fig.dim=c(8,5)}
co2_decomp <- co2_tsb %>%
  model(
    classical_decomposition(co2_ppm, type = "additive")
  ) %>%
  components() %>%
  autoplot() +
  labs(title = "Classical additive decomposition")

co2_stl <- co2_tsb %>%
  model(
    STL(co2_ppm ~ trend() + season(), robust = TRUE)) %>%
  components() %>%
  autoplot() + 
  labs(title = "STL decomposition")

co2_decomp | co2_stl
```

Clearly, both the Classical Decomposition and the STL Decomposition call out the trend line and the seasonality. There is an upward, monotonically increasing, trend. The seasonality is also clearly visible across years. The earlier graph titled **Annual Seasonality** showed a pattern of seasonality that repeats every year. The plots above is a different way to look at the seasonality. Suffice to say that we see a clear seasonal pattern. The random/remainder component look like white noise. Let's conduct a test to see if indeed these are normally distributed.

```{r check if the remainder in white noise, warning=FALSE, echo=FALSE, fig.dim=c(8,4)}
# check with qqplot
decomp_rand <- co2_decomp$data %>% 
  filter(.var == "random")
stl_reminder <- co2_stl$data %>%
  filter(.var == "remainder")

par(mfrow=c(1, 2))
qqnorm(decomp_rand$.val, main="QQ-Plot Decomposition") 
qqline(decomp_rand$.val)

qqnorm(stl_reminder$.val, main="QQ-plot STL") 
qqline(stl_reminder$.val)
```

The QQ plots above show that in the case of Classical Decomposition we do see a the random component being normally distributed. The same isn't quite true with the STL Decomposition. While majority of the remainder values align on the straight line we do see outliers and deviations. We can do "Shapiro Test" to get a quantitative feel for the normal distribution.

```{r additional checks for nornamity, warning=FALSE, echo=TRUE}
# check if remainder (STL) & random (decomposition) are normally distributed
shapiro.test(decomp_rand$.val)
shapiro.test(stl_reminder$.val)
```

The Shapiro test for the case of Classical Decomposition shows normality. The P-value is well above the 0.05 for a 95% confidence level. Again, the same can't be concluded of the STL test. According to the test the remainder isn't normally distributed. The mean values for both Classical Decomposition (`r mean(decomp_rand$.val, na.rm=TRUE)`) and STL Decomposition (`r mean(stl_reminder$.val, na.rm=TRUE)`) are both close to zero. We can further test if the random/remainder component has seasonality. 

```{r check 1 for stationarity, warning=FALSE, echo=TRUE}
# augmented Dickey-Fuller test to check for stationarity
x <- decomp_rand$.val
x <- x[!is.na(x)]
adf.test(x)

# phillips-peron test
x <- stl_reminder$.val
x <- x[!is.na(x)]
pp.test(x)
```

The results of the Augmented Dickey-Fuller test and the Phillips-Peron test confirm that the random/remainder components are stationary - p-values in both cases are 0.01, which less that 0.05, for a 95% confidence level.

We now turn our attention to the stationarity of the observations itself. We test this with KPSS (Kwiatkowski) and PP (Phillips-Peron) methods which check for unit roots for a time-series process. The presence of unit root renders the process non-stationary. 

```{r check 2 for stationarity, echo=FALSE}
test_stat <- c()
test_type <- c()
p_value <- c()
x <- co2_tsb %>% features(co2_ppm, unitroot_kpss)
test_type <- c(test_type, "KPSS")
test_stat <- c(test_stat, x$kpss_stat)
p_value <- c(p_value, x$kpss_pvalue)

x <- co2_tsb %>% features(co2_ppm, unitroot_pp)
test_type <- c(test_type, "PP")
test_stat <- c(test_stat, x$pp_stat)
p_value <- c(p_value, x$pp_pvalue)

data.frame(test_type, test_stat, p_value)
```

The test results show that we've a process that merits considerations as being stationary. The p-value for the case KPSS gives us a 99% confidence level, where as the PP test gives a 90% confidence. 

We can check to see if difference series provides additional insight and/or makes the date more amenable to a suitable model. We apply difference at 12th lag to remove seasonality and retest updated time-series using unit root test.

```{r Differencing at 12th lag, echo=FALSE, warning=FALSE, fig.dim=c(8,3)}
# Apply difference at 12th lag to remove annual seasonality
co2_annual<- co2 %>%
  diff(lag=12) %>%
  ggtsdisplay(lag.max=48)
```

```{r Differencing Histogram, echo=FALSE, warning=FALSE, fig.dim=c(8,3)}
co2_tsb %>%
  ggplot(aes(x=difference(co2_ppm, 12))) +
  geom_histogram(binwidth = 0.10, color="black", fill="grey") +
  labs(x = 'Annual season differencing')
```

While the histogram above looks approximately bell-shaped the ACF and PACF plots still show seasonality. We can do an additional check with KPSS and PP tests as above on the differenced data.


```{r unitroot test on difference, echo=FALSE}
test_stat <- c()
test_type <- c()
p_value <- c()
x <- co2_tsb %>% features(difference(co2_ppm, 12), unitroot_kpss)
test_type <- c(test_type, "KPSS")
test_stat <- c(test_stat, x$kpss_stat)
p_value <- c(p_value, x$kpss_pvalue)

x <- co2_tsb %>% features(difference(co2_ppm, 12), unitroot_pp)
test_type <- c(test_type, "PP")
test_stat <- c(test_stat, x$pp_stat)
p_value <- c(p_value, x$pp_pvalue)

data.frame(test_type, test_stat, p_value)
```

When we difference the data we do see some improvement. The p-values reported by the checks above are consistent and provide a 99% confidence level. We stretch ourselves a bit more to see if the second-order difference provides any additiona improvements. We repeat the tests above.

```{r 2nd order difference of 1 lag, warning=FALSE, echo=FALSE, fig.dim=c(8,3)}
# Apply 2nd order difference of 1 lag to check stationarity
co2_annual_plus1<- co2 %>%
  diff(lag=12) %>%
  diff(lag=1) %>%
  ggtsdisplay(lag.max=48)
```

```{r 1st Lag Histogram, echo=FALSE, warning=FALSE, fig.dim=c(8,3)}
co2_tsb %>%
  ggplot(aes(x=difference(difference(co2_ppm, 12),1))) +
  geom_histogram(binwidth = 0.10, color="black", fill="grey") +
  labs(x = '2nd order differencing')
```

```{r lag1 2nd order difference, echo=FALSE}
test_stat <- c()
test_type <- c()
p_value <- c()
x <- co2_tsb %>% 
  features(difference(difference(co2_ppm, 12), 1), unitroot_kpss)
test_type <- c(test_type, "KPSS")
test_stat <- c(test_stat, x$kpss_stat)
p_value <- c(p_value, x$kpss_pvalue)

x <- co2_tsb %>% 
  features(difference(difference(co2_ppm, 12), 1), unitroot_pp)
test_type <- c(test_type, "PP")
test_stat <- c(test_stat, x$pp_stat)
p_value <- c(p_value, x$pp_pvalue)

data.frame(test_type, test_stat, p_value)
```

Difference of difference time-series looks reasonably stationary around mean of 0. Histogram of second order difference is nearly normal in  distribution and gives confidence that data is centered around mean or in other words, stationary. Unit root test results confirms that series is now stationary at 90% or 99% confidence level. ACF plots still shows out of significance bound correlations up to 12th lag but follow up lags are well within bounds. PACF also reflects sinusoidal pattern indicating residual seasonality.

Overall, we can proceed with model building, as modified time series seems reasonably stationary based upon p-value and we would prefer not to over fit our model.

# Models and Forecasts 
While these plots might be compelling, it is often challenging to learn the exact nature of a time series process from only these overview, "time vs. outcome" style of plots. In this section, we present and evaluate two classes of models to assess which time series model is most appropriate to use. 

## Linear Models 

To begin, we can consider a naive model of the form: 

\begin{equation}
\label{eq:one}
\text{CO}_{2} = \phi_{0} + \phi_{1}t + \epsilon_{t}
\end{equation} 

The above model is essentially a linear function of time and the mean of this model is $\phi_0 + \phi_1t$. Clearly, we know that this is not the case. This model doesn't capture the seasonality that we see in the $CO_2$ observations. Recall that $CO_2$ level peaks in mid-May to mid-June time frame, and hits a low in mid-September to mid-October time frame. We can consider the yearly average value and regress that as a function of time. The new model will look like

\begin{equation}
\label{eq:two}
\text{yearlyCO}_{2} = \phi_{0} + \phi_{1}t + \epsilon_{t}
\end{equation} 

```{r create yearly }
temp <- co2_tsb 
temp$year_ind <- year(co2_tsb$index)
co2_yearly <- aggregate(temp["co2_ppm"], by=temp["year_ind"], mean)

min_year <- min(co2_yearly$year_ind)
lm_yearly_co2 <- lm(co2_ppm ~ I(year_ind - min_year), data=co2_yearly)
summary(lm_yearly_co2)
```

The model above looks reasonable. What we see is that the base is approximately `r round(as.numeric(coef(lm_yearly_co2)[1]))` ppm and an increase of `r round(as.numeric(coef(lm_yearly_co2)[2]))` every year. The p-values are low enough for us to consider this a reasonable model. The model, however, doesn't capture the seasonality and other nuances that may be better modeled with a time series. We will use time-series aware linear models to see if we get additional insights. The model we're exploring is

\begin{equation}
\label{eq:three}
\text{CO}_{2} = \phi_{0} + \phi_{1}*trend + \epsilon_{t}
\end{equation} 

```{r time-series linear model}
co2_1997_linear <- co2_tsb %>%
  model(TSLM(co2_ppm ~ trend()))
report(co2_1997_linear)
```

It shouldn't come as a surprise that $TLSM()$ fitted a model with identical coefficients as that of the $lm()$ method earlier. The trend is essentially yearly. The way we invoked $lm()$, based on yearly average, gives us the same results as $TSLNM()$.

Now, let's compare the fit with the original.

```{r Linear Model and Original Time Series plot, echo=FALSE, fig.dim=c(8,3)}
augment(co2_1997_linear) %>%
  ggplot(aes(x=index)) +
  geom_line(aes(y=.fitted, colour = 'LinearFit')) +
  geom_line(aes(y=co2_ppm, colour = 'Original')) +
  labs(title=TeX(r'($CO_2$ parts per million - data Vs prediction)')) +
  scale_colour_manual(values = c(LinearFit = 'red', Original = 'blue')) +
  guides(colour = guide_legend(title = 'Legend'))
```

```{r Residuals Plot, echo=FALSE, fig.dim=c(8,3)}
co2_1997_linear %>% 
  gg_tsresiduals() +
  labs(title = "Residuals of a Linear Model fit")
```

As expected the linear prediction model is a good fit. The plots above corroborates the finding.

We can examine if data transformation, to logarithmic values, improves the model. As we did for the linear case we'll examine the output of the log-transformed model, and the residuals. The analytical model is

\begin{equation}
\label{eq:four}
\text{log(}{CO}_{2}{)} = \phi_{0} + \phi_{1}t + \epsilon_{t}
\end{equation} 

```{r Logarithmic Transformation}
co2_1997_log <- co2_tsb %>%
  model(TSLM(log(co2_ppm) ~ trend()))
report(co2_1997_log)
```

Logarithmic transformation has reduced the magnitude of the coefficients. This is to be expected as log transformations tends to grow slower than linear transformation. Consequently, the intercept and the slope show smaller values.

```{r Log Model with Original Time Series plot, echo=FALSE, fig.dim=c(8,3)}
augment(co2_1997_log) %>%
  ggplot(aes(x=index)) +
  geom_line(aes(y=.fitted, colour = 'LogFit')) +
  geom_line(aes(y=co2_ppm, colour = 'Original')) +
  labs(title=TeX(r'($CO_2$ parts per million - logarithmic transformation)')) +
  scale_colour_manual(values = c(LogFit = 'red', Original = 'blue')) +
  guides(colour = guide_legend(title = 'Legend'))
```

``` {r Logarithm Model Residuals plots, echo=FALSE, fig.dim=c(8,3)}
co2_1997_log %>% 
  gg_tsresiduals() +
  labs(title = "Residuals of a Logarithmic Model fit")
```

Residual plot convey information similar to what we saw earlier.

As for linear model, we will extend to see the impact of quadratic terms.

\begin{equation}
\label{eq:five}
\text{CO}_{2} = \phi_{0} + \phi_{1}t + \phi_{2}t^2 + \epsilon_{t}
\end{equation} 


```{r Quadratic Model, echo=TRUE}
co2_1997_quad <- co2_tsb %>%
  model(TSLM(co2_ppm ~ trend() + I(trend()^2)))
report(co2_1997_quad)
```

While the quadratic term is statistically significant the coefficient associated with the quadratic term is quite small. Thus, the contribution from the quadratic term can be ignored, without significantly impacting the model. The following plots show the fit Vs actual data and the residual characteristics

```{r Quadratic Model and Original Time Series plot, echo=FALSE, fig.dim=c(8,3)}
augment(co2_1997_quad) %>%
  ggplot(aes(x=index)) +
  geom_line(aes(y=.fitted, colour = 'QuadFit')) +
  geom_line(aes(y=co2_ppm, colour = 'Original')) +
  labs(title=TeX(r'($CO_2$ parts per million - quadratic model)')) +
  scale_colour_manual(values = c(QuadFit = 'red', Original = 'blue')) +
  guides(colour = guide_legend(title = 'Legend'))
```

```{r Quadratic Model Residuals plots, echo=FALSE, fig.dim=c(8,3)}
co2_1997_quad %>% 
  gg_tsresiduals() +
  labs(title = "Residuals of a Quadratic Model fit")
```

At this stage including higher order polynomials may not get us anything more, and further, may be lead to an over-fitted model. The predictions from these models could deviate from earlier model due to the lack of generality in the model.

As a next step we include seasonality to the quadratic model.

```{r quadratic model with season, echo=TRUE}
co2_1997_quad_Final <- co2_tsb %>%
  model(TSLM(co2_ppm ~ trend() + I(trend()^2) + season()))
report(co2_1997_quad_Final)
```

```{r Quadratic Model with seasonal Vs original, echo=FALSE, fig.dim=c(8,3)}
augment(co2_1997_quad_Final) %>%
  ggplot(aes(x=index)) +
  geom_line(aes(y=.fitted, colour = 'QuadModel')) +
  geom_line(aes(y=co2_ppm, colour = 'Original')) +
  labs(title=TeX(r'($CO_2$ parts per million - quadratic model with season)')) +
  scale_colour_manual(values = c(QuadModel = 'red', Original = 'blue')) +
  guides(colour = guide_legend(title = 'Legend'))
```

```{r Quadratic Model with Seasonality Residuals plots, echo=FALSE, fig.dim=c(8,3)}
co2_1997_quad_Final %>% 
  gg_tsresiduals() +
  labs(title = "Residuals of a Quadratic Model fit, with Seasonality")
```

The model is a good fit. We see that the predicted values follow the original data quite faithfully. Given that only the quadratic term is included we do not believe that this model is an over- fit. The residuals appear normally distributed around mean `r mean(augment(co2_1997_quad_Final)$.resid)`. Let us do a forecast and see what we get.

```{r Forecast using Quad Model with Seasonality, echo=TRUE}
quad_model_forecast <- forecast(co2_1997_quad_Final, h=294)
```

```{r forecast plot, echo=FALSE, fig.dim=c(8,3)}
quad_model_forecast %>%
  autoplot(co2_tsb) +
  labs(y = TeX(r'($CO_2$ parts per million)'), 
       title = "Forecast till present using Quadratic + Season Model Fit")
```

The forecast appears in line with what we may expect given the trend and seasonality of the observations we have seen thus far. We believe that this is a reasonable forecast. The last six values from the forecast are `r tail(quad_model_forecast$.mean)`. With forecast set till June 2022 (h=294, being the number of months from Janurary 1998 on), we see the linear model predicting a value of 420 for the first time in April 2022. We need to match with observations that are being measured in 1998 and beyond to validate the effectiveness of the model.

## ARIMA Models 

```{r ARIMA models setup, echo=FALSE}
model_orders <- data.frame(permutations(n = 16, r = 2, v = 0:15, 
                                  set = FALSE, repeats.allowed = TRUE))

colnames(model_orders) <- c("p","q")
model_orders <- model_orders %>% filter(p + q <= 15 & p + q > 0)

aic_bic_scores <- model_orders %>% 
  dplyr::rowwise() %>% 
  mutate(family = ifelse(q == 0, "AR", ifelse(p == 0, "MA", "ARMA")))
```

```{r init variables, echo=FALSE}
aic <- c()
bic <- c()
d <- 1
```

```{r ARIMA models iteration, echo=TRUE}
for (i in 1:nrow(aic_bic_scores)) {
  p <- aic_bic_scores$p[i]
  q <- aic_bic_scores$q[i]
  aic <- c(aic, try_default(AIC(Arima(co2_tsb$co2_ppm,
                                      order = c(p, 1, q), 
                                seasonal=list(order = c(0, d ,0), 12))), 
                            default = NA, quiet = TRUE))
  bic <- c(bic, try_default(BIC(Arima(co2_tsb$co2_ppm,
                                      order = c(p, 1, q), 
                                seasonal=list(order = c(0, d, 0), 12))), 
                            default = NA, quiet = TRUE))
}
```

```{r create the dataframe, echo=FALSE}
aic_bic_scores$aic <- aic
aic_bic_scores$bic <- bic

aic_bic_scores <- aic_bic_scores %>%
  filter(!is.na(aic))
```

```{r plot the aic, bic score, echo=FALSE, fig.dim=c(8,3)}
par(mfrow = c(1, 2))
boxplot(aic_bic_scores$aic ~ aic_bic_scores$family, xlab = "Model family",
        ylab = "AIC", main = "AIC score per model family")
boxplot(aic_bic_scores$bic ~ aic_bic_scores$family, xlab = "Model family",
        ylab = "BIC", main = "BIC score per model family")
```

We ran an ARIMA model with difference (d) set to zero. We iterated over several values of AR order (p) and MA order (q). The box plot above shows the AIC and the BIC values from the iterations. We clearly see that the ARMA model gives the optimal fit. 

```{r examine AIC and BIC scores, echo=FALSE}
aic_bic_scores <- aic_bic_scores %>% arrange(aic, bic)
head(aic_bic_scores, 10)
```
 
As is evident from the AIC and BIC scores reported ARIMA(6,1,9) scores the best. In addition, we'll consider the next two model also - ARIMA(13,1,1) and ARIMA(12,1,3). We see data for each month are well correlated across years. The earlier plot titled **Seasonal Sub-series for each month** shows that when we consider data for a given month across all years we see an upward trend. 
 
```{r AICC Criteria, echo=TRUE}
model_aic<-co2_tsb %>%
  model(ARIMA(co2_ppm ~ 1 + pdq(0:14,0:2,0:5) + PDQ(0,0,0), ic="aic", 
              stepwise=F, greedy=F))

model_aicc<-co2_tsb %>%
  model(ARIMA(co2_ppm ~ 1 + pdq(0:14,0:2,0:5) + PDQ(0,0,0), ic="aicc", 
              stepwise=F, greedy=F))

model_bic<-co2_tsb %>%
  model(ARIMA(co2_ppm ~ 1 + pdq(0:14,0:2,0:5) + PDQ(0,0,0), ic="bic", 
              stepwise=F,greedy=F))
```
```{r model report, echo=FALSE}
cat("-----AIC Model Report-----------------------------------------------\n")
model_aic %>%
  report()
cat("-----AICc Model Report----------------------------------------------\n")
model_aicc %>%
  report()
cat("-----BIC Model Report-----------------------------------------------\n")
model_bic %>%
  report()
```

When we include the difference term we find that the model is tuning itself to ARIMA(2, 1, 4). This likely happens because when we introduce difference term it tends to take away the trend and/or seasonality. The ARIMA(2, 1, 4) is a good fit for the difference time-series for all three information criteria.

```{r model review, echo=FALSE, fig.dim=c(8,3)}
model_bic %>%
  augment() %>%
  ACF(.resid) %>%
  autoplot()
```

### SARIMA Model
Here we consider a full SARIMA model with seasonality and iterate through the values for parameters, keeping the difference parameters (d and D) as 1. We then want to consider models with highest p-value from the Ljung-Box test first, and then choose those models which have lower AIC/BIC values.

```{r arima with seasonality for best ARMA models, echo=FALSE}
model_pdq <- data.frame(permutations(n = 7, r = 2, v = c(0, 1, 2, 3, 6, 9, 12), 
                                  set = FALSE, repeats.allowed = TRUE))

colnames(model_pdq) <- c("p","q")
model_pdq <- model_pdq %>% filter(p + q <= 12 & p + q > 0)

model_PDQ <- data.frame(permutations(n=3, r=2, v=0:2, set = FALSE, 
                                          repeats.allowed = TRUE))
colnames(model_PDQ) <- c("P", "Q")
model_PDQ <- model_PDQ %>% filter(P + Q <= 2)

aic <- c()
bic <- c()
lBox <- c()
order_pdq <- c()
order_PDQ <- c()

for (i in 1: nrow(model_pdq)) {
  p <- model_pdq$p[i]
  q <- model_pdq$q[i]
  for (j in 1: nrow(model_PDQ)) {
    P <- model_PDQ$P[j]
    Q <- model_PDQ$Q[j]
    model_sarima <- try_default(Arima(co2_tsb$co2_ppm, order=c(p, 1, q),  
                          seasonal=list(order=c(P, 1, Q), 12), method="ML"),
                          default=NA, quiet=TRUE)
    
    aic <- c(aic, try_default(AIC(model_sarima), default=NA, quiet=TRUE))
    bic <- c(bic, try_default(BIC(model_sarima), default=NA, quiet=TRUE))
    lBox <- c(lBox, try_default(Box.test(model_sarima$residuals,
                                         type="Ljung-Box")$p.value, 
                                default = NA, quiet = TRUE))
    order_pdq <- c(order_pdq, paste(p, 1, q, sep="-"))
    order_PDQ <- c(order_PDQ, paste(P, 1, Q, sep="-"))
  }
}

modelPerf_df <- data.frame(order_pdq, order_PDQ, aic, bic, ljung_box = lBox)
modelPerf_df <- modelPerf_df %>% arrange(desc(ljung_box))

head(modelPerf_df, 10)
```

Among the models, the one with highest p-value from the Ljung-Box test is the $SARIMA(6,1,3)(1, 1,1)_{12}$ model. We'll use this model to run the forecast. 

## Forecasts 

```{r prediction, echo=TRUE}
Model_Forecast <- co2_tsb %>%
  model(ARIMA(co2_ppm ~ 0 + pdq(6,1,3) + PDQ(1,1,1), stepwise=FALSE, 
              approximation=FALSE)) %>%
  forecast(h = 1236)
```

```{r plot the forecast, echo=FALSE, fig.dim=c(8,3)}
# Comparison
co2_tsb %>% autoplot(co2_ppm) +
  autolayer(Model_Forecast, level=95, alpha=0.5) +
  theme_minimal() +
  labs(title="ARIMA Model Forecast to Present")
```

Given that we have fitted a model, we can make predictions from that model. Our preferred model, named in \autoref{eq:one} is quite simple, and as you might notice, does not in fact match up with the model that we have fitted. 

The forecast is done up to December 2100. The 1st time we hit 420 ppm, is `r match(c(420), round(Model_Forecast$.mean))`, which corresponds to year 2034 and month March. We hit 500 ppm in `r match(c(500), round(Model_Forecast$.mean))`, which corresponds to August of 2089. We do hit 420 ppm a few months later too. Similarly, we hit 500 ppm September of 2089 and a few months after that too. It is worth noting that the distribution of $CO_2$ in March 2234 is normal.

```{r co2 distribution, echo=TRUE}
Model_Forecast$co2_ppm[match(c(420), round(Model_Forecast$.mean))]
```

The model predics that $CO_2$ is a Normal Distribution with a mean of 420 and standard deviation ($\sigma$) of 9.3. Thus, the actual value is likely between 401 to 439 with 95% confidence $(420 - 2*9.3$, $420 + 2*9.3)$. 

# Conclusions 

In this report we started with an analysis of the data (commonly referred to as Exploratory Data. Analysis, or EDA) to see what we can learn of the observations from MLO. We then progressively added additional parameters to fit a model and do the forecast. We started with a naive model of $CO_2$ being a linear function of time. We abandoned this idea because we clearly see seasonality. Then we added time and season parameters to the linear model and used the time-series version (TSLM function) to get a model. We got a linear model that provided a decent approximation to the observations we had. We could get the model fit and forecast. Subsequently, we modeled as an ARIMA model with difference of first order. Then, we introduced seasonality and created a SARIMA model. 

While we can look at the metrics of the model (AIC, BIC, Ljung-Box test etc.) and try to refine the model we don't have a pragmatic way as yet to check the forecast. The one take away is that, at least qualitatively, we see a linear trend in the amount of $CO_2$ in our atmosphere. This alone should give us a cause for concern. We just can't continue business as usual. In a larger sense the exact amount of $CO_2$ that we'll see in the atmosphere is less important than the qualitative trend line. We also see the role the plants and forest play in absorbing $CO_2$. If this report is able to encourage readers to reduce the amount of $CO_2$ we emit we believe that the large goal is achieved. We can and will continue to refine the model as more data comes in to get a good enough model, which is what is realistically achievable.

\newpage

## Introduction
abstract: | 
 The message from the latest Intergovernmental Panel on Climate Change (IPCC) report released in April 2022 on the topic of climate change mitigation is clear: urgent and drastic action is needed if we are to limit global warming to 1.5°C. Fossil fuel based Bitcoin Mining, electricity usage for cellphone and computer, industrial pollution, water contamination, and others play a critical role in the global rise in temperature. IPCC has been examining these trends for more than sixty years. The incessant use of electric gadgets and the rising attraction towards mining digital currencies enforce us to solve the problems in newer ways like a circular economy, carbon capture and storage and greener cities. This report adds new findings and alarming predictions for the next decade.
output: pdf_document
---

The data set is pulled from the NOAA Global Monitoring Laboratory website for the monthly average CO2 levels in Mauna Loa. Data from March 1958 through April 1974 have been obtained by C.David Keeling of the Scripps Insititution of Oceanography. Weekly CO2 values are constructed from daily mean values. NOAA has "confidence that the CO2 measurements made at the Mauna Loa observatory reflect truth about our global atmosphere," given that the observatory is at the summit of Mauna Loa at an altitude of 3400 meters and can measure air masses that are representative of large areas, the measurements are frequently and rigorously calibrated, and ongoing comparisons are made to ensure data accuracy. 
The NOAA GML measures the "mole fraction" of CO2 in dry air, the number of CO2 molecules in a given number of molecules of air after the removal of water vapor. This process is done given that the dry mole fraction reflects the addition and removal of the gas given that dry air does not change when air expands upon heating or ascending to a higher altitude where the pressure is lower. 
The data is missing values for one week in 1954, and that value has been replaced by the average of its surrounding values on each side. Monthly averages are constructed by taking simple averages of values across each month of the time series. 

From plotting the time series of the data, we see a strong upward trend and likely aspects of seasonality given the oscillating pattern of the time series throughout the trend. From the ACF of the CO2, we see high (and statistically significant) correlation for many lags (100 lags shown on the above plot). Significance is maintained through ~250 lags. This high level of autocorrelation is indicative of trended data. 
Observing the PACF plot, we see no statistically significant values past the first lag. 

```{r Data Pull, echo=FALSE, warning=FALSE, fig.dim=c(4,3)}
dataw1 <- read.table("https://gml.noaa.gov/webdata/ccgg/trends/co2/co2_weekly_mlo.txt", header=FALSE)
dataw1 <- dataw1 %>% select(V1, V2, V3, V5) %>% rename("year"=V1, "month"=V2, "day"=V3, "co2"=V5) %>% filter(co2>0)
dataw1$year_month <- lubridate::make_datetime(year=dataw1$year, month=dataw1$month, tz='UTC') 
dataw <- dataw1 %>% select(year_month, co2) %>% mutate(year_month = as.Date(year_month, "%Y-%m"))
dataw <- aggregate(dataw$co2, by=list(year_month=dataw$year_month), FUN=mean) %>% rename("time_index"=year_month, "CO2_avg"=x)
co2_present <- dataw %>% mutate(time_index = yearmonth(as.character(time_index)))  %>% as_tsibble(index=time_index)

co2_present <- tsibble::fill_gaps(co2_present, CO2_avg=330.5)

##basic plots
co2_present %>% gg_tsdisplay(CO2_avg, plot_type="partial") +
  ylab(TeX(r'($CO_2$ parts per million)')) +
  ggtitle("Time Series, Autocorrelation (acf), Partial Autocorrelation(pacf)")

co2_tsb <- co2 %>% as_tsibble()
colnames(co2_tsb)[2] <- "co2_ppm"
```

``` {r examining seasonality, echo=FALSE, warning=FALSE, fig.dim=c(4,3)}
##examining seasonality
co2_present %>% gg_season(CO2_avg) +
  ylab(TeX(r'($CO_2$ parts per million)')) +
  ggtitle("Annual Seasonality")

co2_present %>%
  gg_subseries(CO2_avg) +
  ylab(TeX(r'($CO_2$ parts per million)')) +
  ggtitle("Seasonal Sub-series for each month")

plot.ts(co2_present$CO2_avg, col = 'red', type = 'l', 
     xlab = "Year (time period: month)", ylab = "CO2 ppm", 
     main = TeX(r'(Time-Series plot of $CO_2$ concentration)'))
lines(stats::filter(co2_present$CO2_avg, sides=2, rep(1, 12)/12), lty = 1, 
      lwd = 1.5, col = "blue")
leg.txt <- c("Time-series", 
             "12-Month Symmetric Moving Average")
legend("topleft", legend = leg.txt, lty = c(1, 1, 1), lwd = c(1, 1, 1.5), 
       col = c("red", "blue"), bty = 'n', cex = .8)
```
From the above plots, we observe a strong seasonal trend, persistent across years. 


``` {r decomposition, echo=FALSE, warning=FALSE, fig.dim=c(4,3)}
##decomposition

co2_decomp <- co2_present %>%
  model(
    classical_decomposition(CO2_avg, type = "additive")
  ) %>%
  components() %>%
  autoplot() +
  labs(title = "Classical additive decomposition")
co2_decomp

```
Following the observation of strong seasonality, we can decompose the time series and observe the trend of the data as well as the seasonality. 

``` {r examining just the section above 1979, echo=FALSE, warning=FALSE, fig.dim=c(4,3)}
##examining just the section above 1979

co2_1998_plus <- dataw1 %>% select(year, month, co2) %>% filter(year > 1997)
co2_1998_plus$time_index <- lubridate::make_datetime(year=co2_1998_plus$year, month=co2_1998_plus$month, tz='UTC') 
co2_1998_plus <- co2_1998_plus %>% select(time_index, co2) %>% mutate(time_index = as.Date(time_index))
co2_1998_plus <- aggregate(co2_1998_plus$co2, by=list(time_index=co2_1998_plus$time_index), FUN=mean) %>% rename("CO2_avg"=x)
co2_1998_plus <- co2_1998_plus %>% mutate(time_index = yearmonth(as.character(time_index))) %>% as_tsibble( index=time_index)

co2_1998_plus %>% gg_tsdisplay(CO2_avg, plot_type="partial") +
  ylab(TeX(r'($CO_2$ parts per million)')) +
  ggtitle("Time Series, Autocorrelation (acf), Partial Autocorrelation(pacf)")

```
Examining just the years after 1979, we see a fluctuation of autocorrelations on the ACF plot, dipping below statistical significant at ~75 lags. The PACF plot shows statistically significant values ocurring for the first two lags and then at additional lags further out. 
From the linear model of the co2 levels on the time index, we see a highly statistically significant linear relationship between the two variables, as expected from the trends observed in the time series plot and the results of the ACF plot. 
To examine the detrending data, we will look at the residuals of the trend model. 

``` {r detrending the data, echo=FALSE, warning=FALSE, fig.dim=c(4,3)}
# detrending the data
trend <- lm(co2_present$CO2_avg ~ co2_present$time_index)
summary(trend)
detrend <- residuals(trend)
ts.plot(detrend, xlab="Month", ylab="Detrended C02 Levels")
acf(detrend)
pacf(detrend)
```

Detrending the data we still see the seasonal pattern and somewhat of a quadratic form. We still see highly statistically significant autocorrelation in the ACF plot, and PACF values that remain statistically significant. 

``` {r detrending - differencing, echo=FALSE, warning=FALSE, fig.dim=c(4,3)}
## detrending - differencing

differenced <- diff(co2_present$CO2_avg)
ts.plot(differenced)
acf(differenced)
pacf(differenced)
Box.test(differenced, type="Ljung")
```
The differenced time series appears more like a white noise model. 
Even in the differenced data, we still see statistically significant autocorrelations, which is not indicative of a white noise model. 
The Ljung-Box test has a very small p value, indicating that we should reject the null hypothesis that the residuals of our time series model are independent. 

## Task 2b: Compare linear model forecasts against realized CO2

```{r Compare Linear Model, echo=FALSE, warning=FALSE, fig.dim=c(4,3)}
set.seed(44)
colnames(co2_1998_plus)[2] <- "co2_ppm"

fit <- co2_tsb %>% model(TSLM(co2_ppm ~ trend() + I(trend()^2) + season()))

simulation <- fit %>% forecast(h = 295)
simulation.with.bootstrap <- fit %>% forecast(h = 295, times = 50, bootstrap = TRUE)

aug <- fit %>% augment()
print("Innnovation Residual Mean")
print(mean(aug$.resid))
qqnorm(aug$.resid, pch = 1, frame = FALSE, xlab = "Fitted", ylab = "Residual") 
qqline(aug$.resid)
fit %>% gg_tsresiduals()

p1<-co2_tsb %>%
  ggplot(aes(x = index)) +
  geom_line(aes(y = co2_ppm)) +
  geom_line(aes(y = .mean, colour="red"), data = simulation) +
  labs(title="CO_2 forecast - Linear", y="co_2" ) +
  guides(colour = "none")

p2<-co2_tsb %>%
  ggplot(aes(x = index)) +
  geom_line(aes(y = co2_ppm)) +
  geom_line(aes(y = .mean, colour="red"), data = simulation.with.bootstrap) +
  labs(title="CO_2 forecast with Bootstrap - Linear", y="co_2" ) +
  guides(colour = "none")
grid.arrange(p1,p2, nrow=2)
print("Accuracy of trend + season")
colnames(co2_1998_plus)[1] <- "index"
accuracy(simulation$.mean, co2_1998_plus$co2_ppm)
print("Accuracy of trend + season with Bootstrap")
accuracy(simulation.with.bootstrap$.mean, co2_1998_plus$co2_ppm)

accuracy(simulation, co2_1998_plus,list(winkler = winkler_score), level = 80)
accuracy(simulation.with.bootstrap, co2_1998_plus, list(winkler = winkler_score), level = 80)
accuracy(simulation, co2_1998_plus, list(winkler = winkler_score), level = 95)
accuracy(simulation.with.bootstrap, co2_1998_plus, list(winkler = winkler_score), level = 95)
tail(simulation$.mean)
n <- 36
tail.actual <- tail(co2_1998_plus$co2_ppm, n)
tail.pred <- tail(simulation$.mean, n)
print("RMSE of Last 36 months")
print(sqrt(mean((tail.actual-tail.pred)^2)))

head.actual <- head(co2_1998_plus$co2_ppm, n)
head.pred <- head(simulation$.mean, n)
print("RMSE of First 36 months")
print(sqrt(mean((head.actual-head.pred)^2)))

```
**Linear Model Explanation**

To begin with let's take a look at the residual of the fitted linear model. A 
good forecasting method will yield innovation residuals with the following properties:

1) The innovation residuals are uncorrelated. If there are correlations between 
innovation residuals, then there is information left in the residuals which should 
be used in computing forecasts.

2) The innovation residuals have zero mean. If they have a mean other than zero, 
then the forecasts are biased.The innovation residuals also fails to show us the 
white noise. Thus we anticipate the forecast-ed values to not reflect the full 
absorption of trend and seasonality.

3) The innovation residuals have constant variance. This is known as “homoscedasticity”.

4) The innovation residuals are normally distributed.

Looking at the ACF, there is a strong auto-correlation between the preset values 
of the series and the lagged values. So we conclude the residuals are correlated.
The Innovation mean is close to zero. The innovation residuals have near constant 
variance. The innovation residuals are normally distributed in the histogram plot.

As the residual still have trend/seasonality left in it, while forecasting, the
team explored both with **Residual Re-sampling** via bootstrap residuals and without
boot-straping residuals to explore how well the model behaves. 

As expected the forecast-ed model have a few positives and pitfalls. 

1) The Seasonality is consistently under estimated. This is evident from the height
of the seasonality smaller than the training data in both boot-strapped and 
non-boot-strapped forecasts.

2) The RMSE and other error metrics are reasonably small.

3) To evaluate the distributional forecast accuracy our team employed **Winkler Score** 
test. The Winkler score can be interpreted like an absolute error. For observations 
that fall within the interval, the Winkler score is simply the length of the interval. 
So low scores are associated with narrow intervals. However, if the observation 
falls outside the interval, the penalty applies, with the penalty proportional to 
how far the observation is outside the interval. A score of 2.6 for 80% confidence
interval and 3.28 for 95% confidence interval shows the forecast-ed values are 
not way off.

4) The trend is not captured well. This is evident from the **90.6% increase** in the 
RMSE values for the last 36 months compared to the first 36 months - 1.18415 vs 
0.6213753. Thus, the small unobserved trend in the residuals for the initial forecast-ed 
values create a large effect in the far future forecasts - **the butterfly effect** is the 
sensitive dependence on initial conditions in which a small change in one state of a 
deterministic nonlinear system can result in large differences in a later state.

Based on the above observations, **the best linear model consistently under predicts**.

## Task 3b: Compare ARIMA models forecasts against realized CO2

```{r Compare ARIMA Model, echo=FALSE, warning=FALSE, fig.dim=c(4,3)}
set.seed(44)
colnames(co2_1998_plus)[2] <- "co2_ppm"

fit <- co2_tsb %>% model(ARIMA(co2_ppm ~ 0 + pdq(7,1,8) + PDQ(1,1,1), 
                               stepwise=FALSE, approximation=FALSE)) 
simulation <- fit %>% forecast(h = 295)
simulation.with.bootstrap <- fit %>% forecast(h = 295, times = 50, bootstrap = TRUE)

aug <- fit %>% augment()
print("Innnovation Residual Mean")
print(mean(aug$.resid))
qqnorm(aug$.resid, pch = 1, frame = FALSE, xlab = "Fitted", ylab = "Residual") 
fit %>% gg_tsresiduals()

p1<-co2_tsb %>%
  ggplot(aes(x = index)) +
  geom_line(aes(y = co2_ppm)) +
  geom_line(aes(y = .mean, colour="red"), data = simulation) +
  labs(title="CO_2 forecast - Linear", y="co_2" ) +
  guides(colour = "none")

p2<-co2_tsb %>%
  ggplot(aes(x = index)) +
  geom_line(aes(y = co2_ppm)) +
  geom_line(aes(y = .mean, colour="red"), data = simulation.with.bootstrap) +
  labs(title="CO_2 forecast with Bootstrap - Linear", y="co_2" ) +
  guides(colour = "none")
grid.arrange(p1,p2, nrow=2)
print("Accuracy of trend + season")
colnames(co2_1998_plus)[1] <- "index"
accuracy(simulation$.mean, co2_1998_plus$co2_ppm)
print("Accuracy of trend + season with Bootstrap")
accuracy(simulation.with.bootstrap$.mean, co2_1998_plus$co2_ppm)

accuracy(simulation, co2_1998_plus,list(winkler = winkler_score), level = 80)
accuracy(simulation.with.bootstrap, co2_1998_plus, list(winkler = winkler_score), level = 80)
accuracy(simulation, co2_1998_plus, list(winkler = winkler_score), level = 95)
accuracy(simulation.with.bootstrap, co2_1998_plus, list(winkler = winkler_score), level = 95)

n <- 36
tail.actual <- tail(co2_1998_plus$co2_ppm, n)
tail.pred <- tail(simulation$.mean, n)
print("RMSE of Last 36 months")
print(sqrt(mean((tail.actual-tail.pred)^2)))

head.actual <- head(co2_1998_plus$co2_ppm, n)
head.pred <- head(simulation$.mean, n)
print("RMSE of First 36 months")
print(sqrt(mean((head.actual-head.pred)^2)))
```
**ARIMA Model Explanation**

To begin with let's take a look at the residual of the fitted linear model. A 
good forecasting method will yield the above mentioned innovation residual properties.

ACF strongly reflects white noise. So we conclude the residuals are not correlated.
The Innovation mean is close to zero, but substantially higher than the linear model. 
The innovation residuals have near constant variance. The innovation residuals are 
normally distributed in the histogram plot.

The forecast-ed model is not exhibiting substantial difference to the Linear model.

1) The Seasonality is estimated better than the Linear model. This is evident from the height
of the seasonality relatively closer to the training data in both boot-strapped and 
non-boot-strapped forecasts.

2) The RMSE and other error metrics are high. As a matter of fact, all the error metrics
are higher than the Linear model.

3) To evaluate the distributional forecast accuracy our team employed **Winkler Score** 
test. A score of 37.3 for 80% confidence interval and 47.13 for 95% confidence interval 
shows the forecast-ed values are way off.

4) The trend is not captured well. This is evident from the **1200%** increase in the 
RMSE values for the last 36 months compared to the first 36 months - 16.19685 vs 
1.187938. Thus, the small unobserved trend in the residuals for the initial forecast-ed 
values create a large effect in the far future forecasts - **the butterfly effect**.

Based on the above observations, **the ARIMA model consistently under predicts** and
no better than the linear model predictions.

???Describe how the Keeling Curve evolved from 1997 to the present.???

## Task 4b: Evaluate the performance of 1997 linear and ARIMA models

```{r Evaluate the performance of 1997 linear and ARIMA models, echo=FALSE, warning=FALSE, fig.dim=c(4,3)}
co2_tsb <- co2 %>% as_tsibble()
colnames(co2_tsb)[2] <- "co2_ppm"
arima.fit <- co2_tsb %>% model(ARIMA(co2_ppm ~ 0 + pdq(7,1,8) + PDQ(1,1,1),
                               stepwise=FALSE, approximation=FALSE))
arima.sim <- forecast(arima.fit, model = arima.fit, h=295)
tail(arima.sim$.mean)

linear.fit <- co2_tsb %>% model(TSLM(co2_ppm ~ trend() + I(trend()^2) + season()))
linear.sim <- linear.fit %>% forecast(h = 295)
tail(linear.sim$.mean)

linear.sim.295 <- linear.fit %>% forecast(h = 295)
arima.fit.295 <- arima.fit %>% forecast(h = 295)
all.forecast <- bind_cols(co2_1998_plus, linear.sim.295$.mean, arima.fit.295$.mean)

colnames(all.forecast)[3] <- "linear"
colnames(all.forecast)[4] <- "arima"
forecast.resi <- all.forecast %>%
  mutate(linear.residual = co2_ppm-linear) %>%
  mutate(arima.residual  = co2_ppm-arima)

ttest.resi <- t.test(forecast.resi$linear.residual, 
                     forecast.resi$arima.residual, alternative = c("two.sided"))
print("Residual t-test Result")
print(ttest.resi)

n <- 36
tail.data <- tail(forecast.resi, n)
head.data <- head(forecast.resi, n)
print("RMSE of Last 36 months - ARIMA")
print(sqrt(mean((tail.data$arima.residual)^2)))
print("RMSE of First 36 months - ARIMA")
print(sqrt(mean((head.data$arima.residual)^2)))

print("RMSE of Last 36 months - Linear")
print(sqrt(mean((tail.data$linear.residual)^2)))
print("RMSE of First 36 months - Linear")
print(sqrt(mean((head.data$linear.residual)^2)))

print("Accuracy of Linear Model")
accuracy(forecast.resi$linear, co2_1998_plus$co2_ppm)
print("Accuracy of ARIMA Model")
accuracy(forecast.resi$arima, co2_1998_plus$co2_ppm)

Box.test(forecast.resi$linear.residual, lag = 1, type = c("Ljung-Box"), fitdf = 0)
Box.test(forecast.resi$arima.residual, lag = 1, type = c("Ljung-Box"), fitdf = 0)
```
**Explanation**

???Explain how the model meets 420ppm???

The forecasting performance of the models are assessed by a test and two metrics.

1) **t-test** and **Box test** - t-test determines whether the means of two groups are equal to each other.
True difference in means is not equal to 0 for the residuals of Linear and Arima 
tells us that the models have different means
 ttest H0 => Mean value of 2 residuals are same
Ha=> it is not.
The test shows that the low p-value. mean value of linear residual is lower
linear prediction seems to do a better job. two residuals are statistically different.

Box test is telling the residuals are not independent
There is no independence in the residuals.

2) **The butterfly effect** - It is the sensitive dependence on initial conditions 
in which a small change in one state of a deterministic nonlinear system can result 
in large differences in a later state. A **90.6% increase** in the 
RMSE values for the last 36 months compared to the first 36 months - 1.18415 vs 
0.6213753 of the linear model proves the trend is not fully captured. Thus, 
the small unobserved trend in the residuals for the initial forecast-ed 
values create a large effect in the far future forecasts. Also, a **1200%** increase in the 
RMSE values for the last 36 months compared to the first 36 months - 16.19685 vs 
1.187938 for the ARIMA model echoes the same.

3) **Error Metrics** - ME, RMSE, MAE, MPE, MAPE: ???


```{r 2.5 data, echo=FALSE, warning=FALSE, fig.dim=c(4,3)}
##Seasonally adjust the weekly nOAA data 
##lagging at 12 for annual seasonal, can change to other number for annual "seasons"
season_num <- 12
co2_present$seasonally_adjusted_co2 <- difference(co2_present$CO2_avg, lag=season_num)

##split into seasonally adjusted and nonseasonally adjusted
SA_co2_present <- co2_present %>% select(time_index, seasonally_adjusted_co2)
NSA_co2_present <- co2_present %>% select(time_index, CO2_avg)

##split test and training sets
SA_co2_present_train <- SA_co2_present %>% filter(time_index < yearmonth("2020-08-01"))
SA_co2_present_test <- SA_co2_present %>% filter(time_index > yearmonth("2020-07-01"))

NSA_co2_present_train <- NSA_co2_present %>% filter(time_index < yearmonth("2020-08-01"))
NSA_co2_present_test <- NSA_co2_present %>% filter(time_index > yearmonth("2020-07-01"))


##fit ARIMA models for SA and forecasts

sa_present_fit_best <- auto.arima(y=SA_co2_present_train$seasonally_adjusted_co2, 
                                  max.p=12, max.P=12, max.Q=0, max.d=2, max.q = 0,
                                  max.D=2)

sa_forecast <- forecast(SA_co2_present_train$seasonally_adjusted_co2, 
                        model=sa_present_fit_best, h=24)

plot(sa_forecast)


nsa_present_fit_best <- auto.arima(y=NSA_co2_present_train$CO2_avg, 
                                   max.p=12, max.P=12, max.Q=0, max.d=2, max.q = 0, max.D=2)



nsa_forecast <- forecast(NSA_co2_present_train$CO2_avg, model=nsa_present_fit_best, h=24)

plot(nsa_forecast)

##fits

fit_NSA <- NSA_co2_present_train %>% model(TSLM(CO2_avg ~ trend()+season())) %>% forecast(h="2 years")
fit_SA <- SA_co2_present_train %>% model(TSLM(seasonally_adjusted_co2 ~ trend()+season())) %>% forecast(h="2 years")


##accuracies 

accuracy(fit_NSA, NSA_co2_present_test)
accuracy(fit_SA, SA_co2_present_test)

```


## Question 2.5

```{r 2.5 model, echo=FALSE, warning=FALSE, fig.dim=c(4,3)}
##Seasonally adjust the weekly nOAA data 
##lagging at 12 for annual seasonal, can change to other number for annual "seasons"
season_num <- 12
co2_present$seasonally_adjusted_co2 <- difference(co2_present$CO2_avg, lag=season_num)

##split into seasonally adjusted and nonseasonally adjusted
SA_co2_present <- co2_present %>% select(time_index, seasonally_adjusted_co2)
NSA_co2_present <- co2_present %>% select(time_index, CO2_avg)

##split test and training sets
SA_co2_present_train <- SA_co2_present %>% filter(time_index < yearmonth("2020-08-01"))
SA_co2_present_test <- SA_co2_present %>% filter(time_index > yearmonth("2020-07-01"))

NSA_co2_present_train <- NSA_co2_present %>% filter(time_index < yearmonth("2020-08-01"))
NSA_co2_present_test <- NSA_co2_present %>% filter(time_index > yearmonth("2020-07-01"))

```
First, we split the seasonally adjusted and non-seasonally adjusted time series into training and test sets. following, we will find the top ARIMA models by evaluating on AIC, fitting the models both for in sample testing (inclusive of the entire time series) and out of sample testing (fitting only on the training set). 

```{r 2.5 ARIMA, echo=FALSE, warning=FALSE, fig.dim=c(4,3)}
##produce ARIMA models for SA and NSA

seeking_arima <- function(x.ts, maxord, p_start) { 
  best.aic <- 1e8 ##set a low minimum value to iterate against
  p_seq = c()
  d_seq = c()
  q_seq = c()
  aic_seq = c()
  n <- length(x.ts)
  for (p in p_start:maxord[1])
    for (d in p_start:maxord[2])
      for (q in p_start:maxord[3]) {
        p_seq <- c(p_seq, p)
        d_seq <- c(d_seq, d)
        q_seq <- c(q_seq, q)
        fit <- arima(x.ts, order = c(p,d,q), method='CSS')
        fit.aic <- -2 * fit$loglik + (log(n) + 1) * length(fit$coef)
        aic_seq <- c(aic_seq, fit.aic)
        if (fit.aic < best.aic) {
          best.aic <- fit.aic
          best.model <- c(p,d,q)
        }
      }
  df = data.frame(p_seq, d_seq, q_seq, aic_seq)
  return (df[order(aic_seq),])
}

##SA
best.arima.sa.insample <- seeking_arima(SA_co2_present$seasonally_adjusted_co2, maxord = c(12,2,0), 0)
best.sas.insample <- head(best.arima.sa.insample)

best.arima.sa.outsample <- seeking_arima(SA_co2_present_train$seasonally_adjusted_co2, maxord = c(12,2,0), 0)
best.sas.outsample <- head(best.arima.sa.outsample)

best.sas.outsample
best.sas.insample
##NSA
best.arima.nsa.insample <- seeking_arima(NSA_co2_present$CO2_avg, maxord = c(12,1,12), 0)
best.nsas.insample <- head(best.arima.nsa.insample)

best.arima.nsa.outsample <- seeking_arima(NSA_co2_present_train$CO2_avg, maxord = c(12,1,12), 0)
best.nsas.outsample <- head(best.arima.nsa.outsample)
```

Following the model fitting, we will evaluate the performance of the in-sample models. 

```{r SA model with best AIC ARIMA, echo=FALSE, warning=FALSE, fig.dim=c(4,3)}
### SA model with best AIC ARIMA
fit <- SA_co2_present %>% model(ARIMA(seasonally_adjusted_co2 ~ 0 + pdq(best.sas.outsample$p_seq[1],
                                              best.sas.outsample$d_seq[1],
                                              best.sas.outsample$q_seq[1]) + PDQ(1,1,1), 
                             stepwise=FALSE, approximation=FALSE, method='ML')) 
aug<- fit %>% augment()
print(mean(aug$.resid))
qqnorm(aug$.resid, pch = 1, frame = FALSE, xlab = "Fitted", ylab = "Residual") 
fit %>% gg_tsresiduals()

mse_sa_insample <- (accuracy(aug$.fitted, SA_co2_present$seasonally_adjusted_co2)[2])^2
```

Using the top model from our AIC criterion, for seasonally adjusted in sample data, we observe a fit of `r mse_sa_insample`. We also observe that the residuals appear akin to a whitenoise pattern in their ts plot, as well as roughly normally distributed in their histogram. However, we observe a statistically significant value at lag 12 and 24 in the ACF plot. 

```{r, echo=FALSE, warning=FALSE, fig.dim=c(4,3)}
### NSA model with best AIC ARIMA
fit <- NSA_co2_present %>% model(ARIMA(CO2_avg ~ 0 + pdq(best.nsas.insample$p_seq[1],
                                              best.nsas.insample$d_seq[1],
                                              best.nsas.insample$q_seq[1]) + PDQ(1,1,1), 
                             stepwise=FALSE, approximation=FALSE, method='ML')) 

aug<- fit %>% augment()
print(mean(aug$.resid))
qqnorm(aug$.resid, pch = 1, frame = FALSE, xlab = "Fitted", ylab = "Residual") 
fit %>% gg_tsresiduals()

mse_nsa_insample <- (accuracy(aug$.fitted, NSA_co2_present$CO2_avg)[2])^2
```

Using the top model from our AIC criterion, for seasonally adjusted in sample data, we observe a fit of `r mse_nsa_insample`. We also observe that the residuals appear akin to a whitenoise pattern in their ts plot, as well as roughly normally distributed in their histogram. The ACF plot shows statistically insignificant values for up through 24 lags. 



Next we will be looking at out of sample performance of our models. 

```{r, echo=FALSE, warning=FALSE, fig.dim=c(4,3)}
##out of sample
outofsample <- function(p,dd, q, value, len_sample, len_pred) {
  d <- c()
  for (i in 1:len_pred) {
    n <- len_sample + i-1
    fit <- arima(value[1:n], order = c(p,dd, q), method = "CSS")
    val <- predict(fit,1)
    d <- append(d, round(val$pred[1],1))
  }
  start <- len_sample+1
  end <- len_sample + len_pred
  mean((d-value[start:end])^2)
}

mse_sa <- c()
for (i in 1:6) {
  mse_sa <- c(mse_sa, outofsample(p=best.sas.outsample$p_seq[i], d=best.sas.outsample$d_seq[i], q=best.sas.outsample$q_seq[i], value=NSA_co2_present$CO2_avg, len_sample = 555, len_pred=24))
}
best.sas.outsample$mse <- mse_sa
best.sas.outsample

mse_nsa <- c()
for (i in 1:6) {
  mse_nsa <- c(mse_nsa, outofsample(p=best.nsas.outsample$p_seq[i], d=best.sas.outsample$d_seq[i], q=best.sas.outsample$q_seq[i], value=SA_co2_present$seasonally_adjusted_co2, len_sample = 555, len_pred=24))
}
best.nsas.outsample$mse <- mse_nsa
best.nsas.outsample
```

Evaluating the out of sample performance of the top 6 ARIMA models fit to the seasonally adjusted data, we see that the ARIMA model (12, 1, 0) produces the best out of sample performance, with the lowest mean squared error. This model also produces the lowest AIC. 

Evaluating the out of sample performance of the top 6 ARIMA models fit to the seasonally adjusted data, we see that the ARIMA model (11, 0, 9) produces the best out of sample performance, with the lowest mean squared error. This model produced the third highest AIC, but the difference in AIC score from the top ranked model and the third is relatively small. 

Finally we will fit several polynomial models and evaluate performance. As a first step, we will fit a linear polynomial trend model

```{r mse 2.5, echo=FALSE, warning=FALSE, fig.dim=c(4,3)}

## In addition, fit a polynomial time-trend model to the seasonally-adjusted series and compare its performance to that of your ARIMA model.

##

lin_fit <- SA_co2_present_train %>% model(TSLM(seasonally_adjusted_co2 ~ trend()))

report(lin_fit)

lin_fit_forecast <- forecast(lin_fit, h=24)

mse_lin <- (accuracy(lin_fit_forecast$.mean, SA_co2_present_test$seasonally_adjusted_co2)[2]^2)

```

We see statistically significant coefficients at a very low p-value and an r-squared of 0.257, which means that only about a quarter of the variance in the data is explained by the model. The MSE is `r mse_lin` which is higher than the lowest MSE ARIMA out of sample performance of 1.88. 



```{r polynomial, echo=FALSE, warning=FALSE, fig.dim=c(4,3)}

## In addition, fit a polynomial time-trend model to the seasonally-adjusted series and compare its performance to that of your ARIMA model.

##

linlog_fit <- SA_co2_present_train %>% model(TSLM(log(seasonally_adjusted_co2) ~ trend()))

report(linlog_fit)

linlog_fit_forecast <- forecast(linlog_fit, h=24)

mse_loglin <- accuracy(linlog_fit_forecast$.mean, log(SA_co2_present_test$seasonally_adjusted_co2))[2]^2


```

We see statistically significant coefficients at a very low p-value and an r-squared of 0.181, which means that only about a fifth of the variance in the data is explained by the model, worse than the linear model. The MSE is `r mse_loglin` which is higher than the lowest MSE ARIMA out of sample performance of 1.88. 

```{r quad, echo=FALSE, warning=FALSE, fig.dim=c(4,3)}

## In addition, fit a polynomial time-trend model to the seasonally-adjusted series and compare its performance to that of your ARIMA model.

##

quad_fit <- SA_co2_present_train %>% model(TSLM(seasonally_adjusted_co2 ~ trend() + I(trend()^2)))

report(quad_fit)

quad_fit_forecast <- forecast(quad_fit, h=24)

mse_quad <- accuracy(quad_fit_forecast$.mean, SA_co2_present_test$seasonally_adjusted_co2)[2]^2

```

We see statistically significant coefficients for the quadratic term at a very low p-value and an r-squared of 0.181, which means that only about a fifth of the variance in the data is explained by the model, worse than the linear model. The MSE is `r mse_quad` which is lower than the lowest MSE ARIMA out of sample performance of 1.88, indicating that this model may be more accurate at predicting future values than the ARIMA model. 

## How bad could it get?

```{r 2.6, echo=FALSE, warning=FALSE, fig.dim=c(4,3)}
colnames(NSA_co2_present)[2] <- "co2_ppm"
fit <- NSA_co2_present %>% model(ARIMA(co2_ppm ~ 0 + pdq(best.sas.outsample$p_seq[1],
                                              best.sas.outsample$d_seq[1],
                                              best.sas.outsample$q_seq[1]) + PDQ(1,1,1), 
                             stepwise=FALSE, approximation=FALSE)) 

pred.data <- fit %>%
  forecast(h = 1205)

hilo.pred <- pred.data %>%
  hilo(level = c(80,95)) %>%
  unpack_hilo("95%") %>%
  unpack_hilo("80%")

co2_present_lo <- co2_present
colnames(co2_present_lo)[2] <- "co2_ppm"
co2_present_lo %>% autoplot(co2_ppm) +
  autolayer(pred.data, level=95, alpha=0.5) +
  theme_minimal() +
  labs(title="Future Forecast - ARIMA Model")
```
